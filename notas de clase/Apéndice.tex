\chapter{Apéndice}

\section{Convergencia de Variables Aleatorias}
$Sea (X_n)_{n \in \N}$ una sucesión de variables aleatorias con  $F_{X_n}(x)$ su función de distribución. Sea además $X$ variable aleatoria con distribución $F_{X}(x)$. 

\begin{definition}[Convergencia]
Diremos que $X_n$ converge en \textbf{distribución} a $X$ ssi
\[ \lim_{n \rightarrow \infty}F_{X_n}(x) = F_{X}(x) \Leftrightarrow \lim_{n \rightarrow \infty}Pr(X_n \leq x) = Pr(X \leq x)  \] 
\\
Diremos además que $X_n$ converge en \textbf{probabilidad} a $X$ ssi para todo $\epsilon > 0$
\[ \lim_{n \rightarrow \infty} Pr( |X_n - X | > \epsilon)=0 \]
\\
Finalmente, $X_n$ converge \textbf{casi seguramente} a $X$ ssi

\[  
Pr( \lim_{n \rightarrow \infty}X_n  = X ) = 1 
\] 



\end{definition}

Con estas definiciones en mente, enunciaremos a continuación dos teoremas fundamentales para la justificación de muchos resultados del curso de Estadística

\section{Ley de los Grandes Números (LGN)}
Consideremos un ensayo de Bernoulli como lo podría ser el lanzamiento de una moneda cargada con probabilidad $p$ de obtener cara y $1-p$ de obtener sello, supongamos que no conocemos el valor de $p$ pero nos gustaría estimarlo lo más precisamente posible. Un acercamiento natural al problema sería realizar $n$ lanzamientos y promediar los resultados con el fin de recuperar el parámetro $p$, además, la intuición nos dice que entre más lanzamientos realicemos, más cercana debería ser nuestra estimación de $p$ al valor real.

La ley de los grandes números, se encarga de describir el comportamiento del promedio de una sucesión de variables aleatorias a medida que aumentamos la cantidad de repeticiones del evento. Veamos las dos formas del teorema:

\begin{theorem}[Ley débil de los grandes números] Sea $(X_1,\dots X_n)$ muestra aleatoria i.i.d provenientes de una distribución $X$ con valor esperado $\mu$ y varianza $\sigma^2$ finita.
Entonces el promedio definido como 
\[ \overline{X_n}= \frac{X_1+X_2+\dots+X_n}{n}\]
\textbf{converge en probabilidad } a $\mu$, es decir,
\[ \overline{X_n} \xrightarrow[n]{P} \mathbb{E}(X) =  \mu \] 

\end{theorem}

\begin{theorem}[Ley fuerte de los grandes números]
Sea $(X_1,\dots X_n)$ muestra aleatoria i.i.d provenientes de una distribución $X$ con valor esperado $\mu$ y varianza $\sigma^2$ finita.
Entonces 
\[ \overline{X_n} \xrightarrow[n]{c.s} \mathbb{E}(X) \]
Es decir, el promedio muestral converge casi seguramente a la media de la distribución $X$



\end{theorem}

\begin{remark}
Esta última definición, nos indica que si realizamos un evento una cantidad indefinida de veces y esperamos lo suficiente, eventualmente el promedio converge a la media de la distribución salvo que alguna de las muestras se tomen de un evento con probabilidad 0. 
Notar además que la ley fuerte implica la débil pero no al revés. 


\end{remark}

\begin{example}

De nuestro ejemplo del ensayo de Bernoulli, utilizando la ley de los grandes números, obtendríamos que nuestro promedio eventualmente converge a la esperanza de la variable aleatoria Bernoulli que es justamente $p$. 


\end{example}

\section{Teorema Central del Límite (TCL)}

Otro teorema fundamental de convergencia es el llamado teorema central del límite y es la herramienta que nos permite justificar la elección de la distribución normal como un modelo de aproximación de muchas variables aleatorias. Veamos una vez más el ensayo Bernoulli de nuestra moneda, supongamos la lanzamos una cantidad $n$ suficientemente grande, el teorema central del límite nos permite aproximar el promedio de los lanzamientos como una variable aleatoria normal con media igual al valor esperado de nuestro ensayo y con varianza igual a la varianza de una distribución Bernoulli sobre el número $n$ de lanzamientos.
Veamos la definición formal del teorema

\begin{theorem}[Teorema central del límite (TCL)]

Sea $(X_1\dots X_n)$ variables aleatorias i.i.d con media $\mu$ y varianza $\sigma^2$ ambas finitas. Definamos $\overline{X_n}$ como el promedio de estas $n$ variables aleatorias y definamos 
\[ Z_n = \frac{\overline{X_n}-\mu}{\sigma / \sqrt{n}} \]
Entonces $Z_n$ converge en \textbf{distribución} a $\mathcal{N}(0,1)$, es decir 
\[ Z_n \xrightarrow[n]{D} Z \sim \mathcal{N}(0,1) \]


\end{theorem}

\begin{remark}
Usando propiedades de la distribución normal, lo anterior es equivalente a decir que para $n$ suficientemente grande
\[ \overline{X_n} \sim \mathcal{N} \left (\mathbb{E}(X),\frac{\mathbb{V}ar(X)}{n} \right) \]
\end{remark}
\begin{remark}
Existe otra versión del TCL más general en que no se pide que las variables aleatorias sean identicamente distribuidas pero si independientes y se le conoce como Lyapunov TCL
\end{remark}

\begin{example}
Siguiendo el teorema y nuestro ejemplo anterior, podemos concluir que para el caso de $n$ lanzamientos de una variable aleatoria Bernoulli de media $p$ y varianza $p(1-p)$ que 
\[ \overline{X_n} \sim \mathcal{N} \left (p,\frac{p(1-p)}{n} \right ) \]

\end{example}


