\chapter{Estadísticos}

Recordemos que en la aplicación de la estadística, además de nuestros supuestos, solo contamos con \emph{datos}, consecuentemente, todo lo que hagamos partirá desde el uso de éstos. En este sentido, definimos un estadístico como una función de (las realizaciones de) una variable aleatoria, definida desde el espacio muestral. Es decir, cualquier función \emph{medible} de los datos. Recordar que $\Theta$ es el espacio de parámetros.\\ 


\begin{definition}[Estadístico]
\label{def:estadístico}
Sea $(\cT,\cA,\mu)$ un espacio de probabilidad y $X\in\cX$ una variable aleatoria con distribución paramétrica $\cP = \{P_\theta\ \tq\ \theta\in\Theta\}$. Un estadístico es una función medible de la realización $X=x$ independiente del parámetro $\theta$ (y de la distribución $P_\theta$).
\begin{align}
\nonumber
	T:\ &\cX \rightarrow \cT\\
\nonumber
	&x\mapsto T(x).
\end{align} 

\end{definition}

\begin{remark}
Es muy relevante diferenciar el valor del estadístico $T(x)$ como función de los datos (considerados por nosotros como la realización $X=x$ de la variable aleatoria), de la aplicación de la función $T(\cdot)$ a la variable aleatoria $X$, es decir, $T(X)$. El primero es un valor "fijo" mientras que el segundo es una VA con propia distribución de probabilidad inducida por $P_\theta$ y por la función $T$ (llamada distribución \emph{pushforward} $T_{\sharp P_\theta}$).\\ 
\end{remark}


En base a los datos $x$, algunos estadísticos pueden ser: 
\begin{equation}
\nonumber
	T(x) = \frac{1}{n}\sum_{i=1}^nx_i,\qquad T'(x) = x, \qquad T''(x) = \min(x), \qquad T'''(x) = c\in \mathbb{R}.
\end{equation}

\section{Suficiencia}
En términos generales, el objetivo de un estadístico es \textit{encapsular} o \textit{resumir} la información contenida en una muestra (de datos) $x = (x_1,x_2,\ldots,x_n)$ que es de utilidad para determinar (o estimar) un/el/los parámetros de la distribución de $X$ o alguna otra propiedad de ésta. Por esta razón, la función identidad o el promedio parecen cumplir, al menos intuitivamente, con esta misión. Esto es por que se intuitivamente queremos extraer la mayor información posible de la data, esto lo logran el estadístico $T$ (que resume todos los datos) y el estadístico $T'$ (que contiene todos los datos). Por el contrario, notemos que el estadístico $T''$ \emph{pierde información}, dado que solo se extrae el mínimo valor de toda la data obtenida, así, perdiendo la representación de la, e.g., dispersión de la muestra. El mismo análisis se puede hacer para el estadístico constante, el que no contiene  información alguna de los datos.\\

Coloquialmente, la idea de suficiencia de un estadístico (con respecto a un parámetro) puede ser expresada como  
\begin{displayquote}[Ronald Fisher, On the mathematical foundations of theoretical statistics] \it
“…no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter.”
\end{displayquote}

Formalmente, definimos un estadístico mediante. 
\begin{definition}[Estadístico Suficiente]
\label{def:estadístico_suficiente}
Sea $(S,\cA,\mu)$ un espacio de probabilidad y $X\in\cX$ una variable aleatoria con distribución paramétrica $\cP = \{P_\theta\ \tq\ \theta\in\Theta\}$. Diremos que la función $T:\cX\rightarrow\cT$ es un estadístico suficiente para $\theta$ (o para $X$ o para $\cP$) si la ley condicional $X|T(X)$ no depende del parámetro $\theta$, es decir, 
\begin{align}
\nonumber
	P_\theta(X\in A | T(X)),\ A\in\cB(X), \text{no depende de }\theta.
\end{align} 
\end{definition}

Observemos entonces que si $T(X)$ es un estadístico suficiente, entonces, existe una función que

\begin{equation}
\nonumber
	H(\cdot,\cdot): \cB(X)\times\cT \rightarrow [0,1]
\end{equation}
que es una distribución de probabilidad en el primer argumento y es medible en el segundo argumento.\\

Para poder entender mejor el concepto de un Estadístico Suficiente, se dan los siguientes ejemplos:

\begin{example}[Estadístico suficiente trivial]
	\label{ex:suficiencia_trivial}
	Para cualquier familia paramétrica $\cP$, el estadístico definido por
	\begin{equation}
	\nonumber
		T(x) = x
	\end{equation}
es suficiente. En efecto, $P_\theta(X\in A|X=x) = \ind_{A}(x)$ no depende del parámetro de la familia. 
\end{example}

\begin{example}[Estadístico suficiente Bernoulli]
	Sea $x=(x_1,\ldots,x_n) \sim Ber(\theta)$, $\theta \in \Theta = [0,1]$, es decir
	\begin{equation}
	\nonumber
		P_\theta(X=x) = \theta^{\sum x_i}(1-\theta)^{n-\sum x_i}.
	\end{equation}
	Veamos que $T(x) = \sum\limits_{i=1}^{n} x_i$ es un estadístico suficiente (por definición). En efecto
	\begin{alignat*}{3}
		P(X=x|T(X)=t) 	&= \frac{P(T(X)=t| X=x )P( X=x )}{P(T(X)=t)} \quad&&\text{(T. Bayes)}\\
						&= \frac{\ind_{T(x)=t}\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}{\binom{n}{t}\theta^t(1-\theta)^{n-t}} &&\text{(modelo y suma de Bernoulli es Binomial)}\\
						&= \ind_{T(x)=t} \binom{n}{t}^{-1} && \text{(pues $T(x)=t$)}
	\end{alignat*}
	Consecuentemente, $T(x)=\sum\limits_{i=1}^{n} x_i$ es estadístico suficiente.
\end{example}

Intuitivamente, nos gustaría poder verificar directamente de la suficiencia de un estadístico desde la distribución (o densidad) de una VA, o al menos verificar una condición más simple que la definición. Esto es porque verificar la no-dependencia de la distribución condicional $P(X|T)$ puede ser no trivial, engorroso o tedioso. Para esto enunciaremos el Teorema de Fisher-Neyman, el cual primero requiere revisar la siguiente definición. 

\begin{definition}[Familia Dominada]
	Una familia de modelos paramétricos $\familiaparametrica$ es dominada si existe una medida $\mu$, tal que $\forall \theta\in\Theta, P_\theta$ es absolutamente continua con respecto a $\mu$ (denotado $ P_\theta \ll \mu$), es decir, 
	\begin{equation}
	\nonumber
		\forall \theta\in\Theta, A\in\cB(X), \mu(A)=0 \Rightarrow P_\theta(A) = 0.
	\end{equation}
\end{definition}

La definición anterior puede interpretarse de la siguiente forma: si una familia de paramétrica $\familiaparametrica$ es dominada por una medida $\mu$, entonces ninguno de los modelos $P_\theta\in\cP$ puede asignar medida (probabilidad) no nula a conjuntos que tienen medida cero bajo $\mu$ (la medida \textit{dominante}). Una consecuencia fundamental de que la distribución $P_\theta$ esté dominada por $\mu$ está dada por el Teorema de Radon–Nikodym,  el cual establece que si $ P_\theta \ll \mu$, entonces la distribución $P_\theta$ tiene una densidad con respecto a $\mu$, es decir,	
	\begin{equation}
	\nonumber
		\forall A\in\cB(X), P_\theta(X\in A) = \int _A p_\theta(x) \mu(\d x),
	\end{equation}
donde $p_\theta(x)$ es conocida como la densidad de $P_\theta$ con respecto a $\theta$ (o también como la derivada de Radon–Nikodym  $\frac{d P_\theta}{d \mu}$).

Con la noción de Familia Dominada y de densidad de probabilidad, podemos enunciar el siguiente teorema importante y fundamental que conecta la forma de la densidad de un modelo paramétrico con la suficiencia de su estadístico. 

\begin{theorem}[Factorización, Neyman-Fisher]
	\label{teo:neyman-fisher}
	
	Sea $\familiaparametrica$  una familia dominada por $\mu$, con $p_\theta$ la densidad de $P_\theta$. Entonces, $T$ es un estadístico suficiente si y solo si existen funciones apropiadas $g_\theta(\cdot)$ y $h(\cdot)$, i.e., medibles y no-negativas, tal que la densidad $p_\theta$, $\theta\in\Omega$, admite la siguiente factorización: 
	\begin{equation}
		\label{eq:neyman-fisher}
		p_\theta (x) = g_\theta(T(x))h(x).
	\end{equation}

    Lo anterior se debe cumplir $\forall x\in\mathfrak{X}$ y $\mu-ctp$. También, se tiene que esto es condición necesaria y suficiente para decir que $T(X)$ es suficiente.
\end{theorem}

El Teorema de Neyman-Fisher es clave para evaluar, directamente de la densidad de un modelo, la suficiencia de un estadístico. Pues al identificar la expresión de la V.A. que interactúa con el parámetro (en la función $g_\theta$) es posible determinar el estadístico suficiente. Antes de ver una demostración informal del Teorema \ref{teo:neyman-fisher}, revisemos un par de ejemplos.

\begin{example}[Factorización Bernoulli]
	Notemos que la densidad de Bernoulli (que es igual a su distribución por ser un modelo discreto) factoriza tal como se describe en el Teorema \ref{teo:neyman-fisher}. En efecto, consideremos $x=(x_1,\ldots, x_n)\sim$ Bernoulli($\theta$) y el estadístico $T(x) = \sum x_i$, entonces, 
	\begin{equation}
		\mathbb{P}(X=x) = \underbrace{\theta^{\sum x_i}(1-\theta)^{n-\sum x_i}}_{g_\theta(T(x))} \cdot \underbrace{1}_{h(x)}
	\end{equation}
	Con lo anterior, se tiene que $g_\theta(T(x))$ y $h(x)$ cumplen que son medibles no negativas con lo cual se cumplen las hipótesis del Teorema de Neyman-Fisher y entonces $T(X)$ es suficiente
\end{example}

\begin{example}[Factorización Normal (varianza conocida)]
	Consideremos ahora $x=(x_1,\ldots, x_n)\sim$ $\cN(\mu,\sigma^2$), con $\sigma^2$ conocido y el estadístico $T(x) = \frac{1}{n}\sum x_i$, entonces, 
	\begin{align*}
		p(X=x) & = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(x_i-\mu)^2\right)\\
		&=  (2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right)\\
		&=  (2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n((x_i-\bar{x}) + (\bar{x}-\mu))^2\right)\\
		&=  (2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\bar{x})^2 + 2\cancel{(x_i-\bar{x})}(\bar{x}-\mu) + (\bar{x}-\mu)^2\right)\\
		&=  \underbrace{(2\pi\sigma^2)^{-n/2}\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\bar{x})^2\right)}_{h(x)} \underbrace{\exp\left( -\frac{1}{2\sigma^2}\sum_{i=1}^n (\bar{x}-\mu)^2\right)}_{g_\theta(T(x))}
	\end{align*}
	
	Notamos que $h(x)$ no depende de los parámetros y solo depende de los datos, en cambio, $g_\theta(T(x))$ depende del parámetro y es función del estadístico $T(X)$. Nuevamente se cumplen que las funciones anteriores son medibles no-negativas y se cumplen las hipótesis del Teorema de Neyman-Fisher y entonces $T(X)$ es un estadístico suficiente.
\end{example}

A continuación, veremos la prueba del Teorema \ref{teo:neyman-fisher} para el caso discreto. 


\begin{proof}[Demostración de Teorema Neyman-Fisher, caso discreto]
Primero probamos la implicancia hacia la derecha ($\Rightarrow$), es decir, asumiendo que $T(X)$ es un estadístico suficiente, tenemos,
	\begin{alignat*}{3}
		p_\theta(X=x) 	&= P_\theta(X=x, T(X)=T(x))\\
						&= \underbrace{P_\theta(X=x| T(X)=T(x))}_{h(x)\text{, no depende de $\theta$ por hipótesis}} \underbrace{P_\theta(T(X)=T(x))}_{g_\theta(T(x))},
	\end{alignat*}
	es decir, la factorización deseada.

	Ahora probamos la implicancia hacia la izquierda ($\Leftarrow$), es decir, asumimos la factorización en la ecuación \eqref{eq:neyman-fisher}. En primer lugar, tenemos que el modelo se puede escribir como (Bayes)
	\begin{equation*}
		\label{eq:bayes_NF}
		p_\theta(X=x|T(X)=t)=  \frac{p_\theta(T(X)=t|X=x)p_\theta(X=x)}{p_\theta(T(X)=t)}.
	\end{equation*}
	Donde $p_\theta(T(X)=t|X=x)= \ind_{T(x)=t}$ y la hipótesis esto nos permite escribir 
	\begin{alignat*}{3}
		p_\theta(X=x)&=  g_\theta(T(x))h(x)\\
		p_\theta(T(X)=t) &= \sum_{x';T(x')=t}p_\theta(X=x') = \sum_{x';T(x')=t}g_\theta(T(x'))h(x')
	\end{alignat*}
	Incluyendo estas últimas dos expresiones en la ec.~\eqref{eq:bayes_NF}, tenemos 
	\begin{equation}
		\label{eq:NF_final}
		p_\theta(X=x|T(X)=t)=  \frac{\ind_{T(x)=t}\cancel{g_\theta(T(x))}h(x)}{\sum_{x';T(x')=t}\cancel{g_\theta(T(x'))}h(x')}=  \frac{\ind_{T(x)=t}h(x)}{\sum_{x';T(x')=t}h(x')}
	\end{equation}
	donde los términos que se cancelan son todos iguales a $g_\theta(t)$.\\

	Finalmente, como el lado derecho de la ecuación \eqref{eq:NF_final} no depende de $\theta$, se concluye la demostración.
\end{proof}

\subsection{Particiones Suficientes}

Un estadístico induce una partición en el conjunto de \emph{outcomes}  posibles. Es posible estudiar la suficiencia en términos de estas particiones, dadas por $\left \{ x | T(x) = t \right  \}$, para cada $ t $. 

\begin{definition}
    Una partición $ \left \{ B_1,...B_k \right \}$ se dice suficiente si $f(x | x \in B_i) $ no depende de $\theta$. 
\end{definition}
\begin{theorem}
    Un estadístico es suficiente si y sólo si la partición que induce es suficiente.
\end{theorem}
\begin{proof}
Ejercicio. 
\end{proof}

\begin{example}
Sean $X_1,X_2, X_3\sim Bernoulli(\theta)$. Sea $T= \sum_{i} X_i$. En el Cuadro \ref{tab: Part.Bernoulli suficiente} se puede observar los \emph{outcomes} y los estadísticos: 
\newpage
\begin{table}[h]
    \centering
    \begin{tabular}{c  c  c}  
        $x^{n} $  & $t$  & $ p(x|t) $\\ \hline
        $ (0,0,0) $ & $t=0$ & $1$ \\  \hline
       $ (0,0,1) $ & $t=1$ & $1/3$   \\
        $ (0,1,0) $ & $t=1$ & $1/3$ \\
        $ (1,0,0) $  & $t=1$ & $1/3$ \\ \hline 
        $ (0,1,1) $ & $t=2$ & $1/3$   \\
        $ (1,0,1) $ & $t=2$ & $1/3$ \\
        $ (1,1,0) $  & $t=2$ & $1/3$ \\ \hline 
        $ (1,1,1) $ & $t=3$ & $1$ \\
        
        
    \end{tabular}
    \caption{\emph{Outcomes} y estadísticos (Bernoulli, $T= \sum_{i} X_i$)  }
    \label{tab: Part.Bernoulli suficiente}
\end{table}
Notemos que como $p(x | t )$ no depende de $\theta$, $T$ es un estadístico suficiente. 
\end{example}


\begin{remark}
Dos estadísticos pueden generar la misma partición. Por ejemplo $T$ del ejemplo anterior, y $U= 3 T$. 
\end{remark}
\begin{remark}
Toda partición que refine a una partición suficiente será suficiente.
\end{remark}
Veamos un ejemplo de un estadístico que no genera una partición suficiente (y por lo tanto, no es suficiente)
\begin{example}
Sean $X_1,X_2$ y $X_3 \sim Bernoulli(\theta)$. Entonces $T=X_1$ no es suficiente. Veamos su partición en el Cuadro \ref{tab: Part.Bernoulli no-suficiente}: 
\begin{table}[h]
    \centering
    \begin{tabular}{c  c  c}  
        $x^{n} $  & t  & $ p(x|t) $\\ \hline
        $ (0,0,0) $ & $t=0$ & $(1-\theta)^2$ \\  
       $ (0,0,1) $ & $t= 0$ & $\theta(1-\theta)$   \\
        $ (0,1,0) $ & $t=0$ & $\theta(1-\theta)$ \\
        $ (0,1,1) $ & $t=0$ & $\theta^2$   \\ \hline 
        $ (1,0,0) $  & $t=1$ & $(1-\theta)^2$ \\  
        $ (1,0,1) $ & $t=1$ & $\theta(1-\theta)$ \\
        $ (1,1,0) $  & $t=1$ & $\theta(1-\theta)$ \\ 
        $ (1,1,1) $ & $t= 1$ & $\theta^2$ \\
    \end{tabular}
    \caption{\emph{Outcomes} y estadísticos (Bernoulli, $T= X_1$)  }
        \label{tab: Part.Bernoulli no-suficiente}
\end{table}

\end{example}


\section{Suficiencia minimal}
La idea de suficiencia del estadístico dice relación, coloquialmente, con la \textit{información} contenida en el estadístico que permite \textit{determinar} el parámetro $\theta$. En ese sentido, se tiene la intuición que un estadístico es suficiente si no existe otro estadístico que pueda determinar de \textit{mejor} formar el parámetro usando los mismos datos. En el extremo de esta intuición de suficiencia, el estadístico puede ser simplemente todos los datos, i.e, $T(X)=X$ (estadístico trivial), en cuyo caso la suficiencia es directa como se vio en el Ejemplo \ref{ex:suficiencia_trivial}. En esta sección, por el contrario, estamos interesados en estadísticos que son suficientes pero que contienen la mínima cantidad de información, pues considerar todos los datos puede ser redundante en cuanto a la determinación del parámetro.\\ 

Sin una definición formal de \textit{información} aún, recordemos que los estadísticos representan un resumen o una compresión  de los datos mediante la función $T(\cdot)$ medible. En este sentido, la aplicación de dicha función solo puede \textit{quitar} o, a lo sumo, \textit{mantener la información desde la preimagen a la imagen}. Esto nos permite definir el siguiente concepto:

\begin{definition}[Estadístico Suficiente Minimal]
	Un estadístico $T:\cX\rightarrow\cT$ es suficiente minimal si
	\begin{itemize}
		\item $T(X)$ es suficiente, y
		\item $\forall T'(X)$ estadístico suficiente, existe una función $f$ tal que $T(X) = f(T'(X)).$ 
	\end{itemize}
\end{definition}  

\begin{example}
Si $X_1,..X_{2n}$ son observaciones i.i.d de una normal $\mathcal{N}(\theta,1)$, $\theta \in \R$, entonces:
$$
\bar{T}=
\begin{pmatrix}
\sum_{i=1}^{n} X_i\\ 
\sum_{i=n+1}^{2n}X_i
\end{pmatrix}
$$
es suficiente pero no minimal. Se puede demostrar que $T=\sum_{i=1}^{2n} X_i$ es suficente minimal. 
\end{example}

Los estadísticos suficiente minimales están claramente definidos pero dicha definición no es útil para encontrar o construir  estadístico suficiente minimales. El siguiente Teorema establece una condición que permite evaluar si un estadístico es suficiente minimal 

\begin{theorem}[Suficiencia minimal]
	\label{teo:suficiencia_minimal}
	Sea $\familiaparametrica$ una familia dominada con densidades $\densidadparametrica$ y asuma que existe un estadístico $T(X)$ tal que para cada $x,y\in\cX$:
	\begin{equation}
		\frac{p_\theta(x)}{p_\theta(y)} \text{ no depende de }\theta \Leftrightarrow T(x) = T(y)\label{eq:suf_min_thm}
	\end{equation}
	entones, $T(X)$ es suficiente minimal.
\end{theorem}

Antes de probar este teorema, veamos un ejemplo aplicado a la distribución de Poisson. 
\begin{example}
	Recordemos que la distribución de Poisson (de parámetro $\theta$) modela la cantidad de eventos en un intervalo de tiempo de la forma y consideremos las observaciones $x=(x_1,\ldots, x_n)\sim Poisson(\theta)$ con densidad
	\begin{equation}
		p_\theta(x) = \prod_{i=1}^n\frac{e^{-\theta}\theta^{x_i}}{x_i!} = \frac{e^{-n\theta}\theta^{\sum_{i=1}^n x_i}}{\prod_{i=1}^nx_i!}.
	\end{equation}
	Notemos que la razón de estas densidades para dos observaciones $x,y\in\cX$ toma la forma 
		\begin{equation}
		\frac{p_\theta(x)}{p_\theta(y)} = \frac{\theta^{\sum_{i=1}^n x_i - \sum_{i=1}^n y_i}} {\prod_{i=1}^nx_i! / \prod_{i=1}^ny_i!}, 
	\end{equation}
	lo cual no depende de $\theta$ únicamente si $\sum_{i=1}^n x_i  = \sum_{i=1}^n y_i$, consecuentemente, $T(x) = \sum_{i=1}^n x_i$ es un estadístico suficiente minimal de acuerdo al Teorema \ref{teo:suficiencia_minimal}.
\end{example}

\begin{proof}[Demostración de Teorema \ref{teo:suficiencia_minimal}] Recordemos que queremos demostrar que la ec.~ \eqref{eq:suf_min_thm} implica que $T(\cdot)$ es estadístico suficiente minimal. Primero veremos que $T$ es suficiente. Dada la partición inducida por el estadístico $T(X)$, para un valor $x\in\cX$ consideremos $x_T\in\{x'; T(x') = T(x)\}$, entonces
\begin{equation}
 	p_\theta(x) = \underbrace{{p_\theta(x) }/{p_\theta(x_T) }}_{h(x)\text{ indep.  } \theta} \underbrace{p_\theta(x_T) }_{q_\theta(T(x))	}
 \end{equation} 
 donde la no dependencia de $\theta$ se tiene por el supuesto del Teorema. 

 Para probar que el estadístico es suficiente minimal, asumamos que existe otro estadístico suficiente $T'(X)$, y consideremos dos valores en el mismo subconjunto de la partición inducida por $T'(X)$, i.e., $x,y\in\cX,\ \tq\ T'(x) = T'(y)$, y veamos que (mediante la factorización de Neyman-Fisher) podemos escribir la razón de verosimilitudes de la forma

 \begin{equation}
 	\frac{p_\theta(x)}{p_\theta(y)} = \frac{g'_\theta(T'(x))h'(x)}{g'_\theta(T'(y))h'(y)} = \frac{h'(x)}{h'(y)}, \quad \text{pues } T'(x) = T'(y) 
 \end{equation}
 consecuentemente, el enunciado nos permite aseverar que como $\frac{p_\theta(x)}{p_\theta(y)}$ no depende de $\theta$, entonces $T(x) = T(y)$. Es decir, hemos mostrado que $T'(x) = T'(y)$ implica $T(x) = T(y)$, por lo que $T$ es función de $T'$.
	
\end{proof}


\section{La familia exponencial}

Hasta este punto, hemos considerado algunas distribuciones paramétricas, tales como Bernoulli, Gaussiana o Poisson, para ilustrar distintas propiedades y definiciones de los estadísticos. En esta sección, veremos que realmente todas estas distribuciones (y otras más) pueden escribirse de forma unificada. Para esto, consideremos la siguiente expresión llamada \textit{log-normalizador} (la razón de este nombre será clarificada en breve).
\begin{equation}
	\label{eq:lognormalizador}
	A(\eta) = \log\int_\cX \expo{\sum_{i=1}^s\eta_iT_i(x)}h(x)\dx,
\end{equation}
donde definimos lo siguiente:
\begin{itemize}
	\item $\eta = [\eta_1,\ldots,\eta_s]^\top$ es el parámetro natural
	\item $T = [T_1,\ldots,T_s]^\top$ es un estadístico
	\item $h(x)$ es una función no-negativa
\end{itemize}

\begin{definition}[La Familia Exponencial]
    Definamos la siguiente función de densidad de probabilidad parametrizada por $\eta\in\{\eta | A(\eta)<\infty\}$ con $\theta\in\Theta$
\begin{equation}
	\label{eq:densidadexponencial}
 	p_\theta(x) = \expo{\sum_{i=1}^s\eta_i(\theta)T_i(x)-A(\theta)}h(x)
 \end{equation} 
 donde el hecho que $p_\eta(x)$ integra uno puede claramente verificarse reemplazando la ecuación \eqref{eq:lognormalizador} en \eqref{eq:densidadexponencial}, con lo cual se puede ver que $A$ definido en \eqref{eq:lognormalizador} es precisamente el logaritmo de la constante de normalización de la densidad definida en la ec.~ \eqref{eq:densidadexponencial}. Equivalemente, y comunmente, se utiliza que la familia exponencial viene dada por funciones de densidad de la forma:
 
 \[p_\theta(x) = \expo{\sum_{i=1}^s\eta_i(\theta)T_i(x)}h(x)g(\theta)\]
 
 donde $g$ es una función no-negativa.
\end{definition}

 Muchas de las distribuciones que usalmente consideramos pertenecen a la familia exponencial, por ejemplo, la distribución normal, exponencial, gamma, chi-cuadrado, beta, Dirichlet, Bernoulli, categórica, Poisson, Wishart (inversa) y geométrica. Otras distribuciones solo pertenecen a la familia exponencial para una determinada elección de sus parámetros, como lo ilustra el siguiente ejemplo.

\begin{example}[El modelo binomial pertenece a la familia exponencial]
Recordemos la distribución binomial está dada por 

\begin{align*}
	\bin{x|\theta,n} 	&=\binom{n}{x}\theta^x(1-\theta)^{n-x},\quad x\in\{0,1,2\ldots,n\}\\
					&=
					\underbrace{\binom{n}{x}}_{h(x)}\expo{x\underbrace{\loga{\frac{\theta}{1-\theta}}}_{\text{parámetro natural}} + \underbrace{n\loga{1-\theta}}_{-{A(\theta)}}}\\
\end{align*}
consecuentemente, para que $h(x)$ sea únicamente una función de la variable aleatoria, entonces el número de intentos $n$ tiene que se una cantidad conocida, \textbf{no un parámetro}. 	
 \end{example} 

\begin{example} [El modelo normal pertenece a la familia exponencial]
La distribución normal $\mathcal{N}(\mu,\sigma^{2})$ tiene densidad:

\begin{align}
    p_{\theta}(x) = &  \dfrac{1}{\sqrt{2\pi \sigma^{2}}} e^{\frac{-1}{2\sigma^{2}}(x-\mu)^{2}}\\
    & = \dfrac{1}{\sqrt{2\pi}}\exp\left(\dfrac{\mu}{\sigma^{2}}x -\frac{-1}{2\sigma^{2}}x^{2} - \left(\dfrac{\mu^{2}}{2\sigma^{2}} + \log \sigma \right)\right)
\end{align}
donde $\theta = (\mu, \sigma^{2})$. Esta es una familia exponencial de dos parámetros con
\begin{itemize}
    \item Estadístico: $T_{1}(x) = x$, $T_{2} (x) = x^2$,
    \item Parámetro natural: $\nu_{1}(\theta) = \mu/\sigma^{2}$, $\nu_{2}(\theta) = -1/(2\sigma^{2})$,
    \item $A(\theta) = \mu^{2}/(2\sigma^{2}) + \log \sigma $,
    \item $h(x) = 1/\sqrt{2\pi}$.
\end{itemize}  

\end{example}

\begin{example} \textbf{Ejercicio} Demuestre que las distribuciones \emph{Poisson} y \emph{Bernoulli} pertenecen a la familia exponencial. 

\end{example}

\begin{remark}
 El estadístico $T$ es un estadístico suficiente para $\nu$ en la familia exponencial. En efecto, notemos que 
 \begin{equation}
	\label{eq:densidadexponencial2}
 	p_\theta(x) = \underbrace{\expo{\sum_{i=1}^s\eta_i(\theta)T_i(x)-A(\theta)}}_{g_\theta(T(x))}\underbrace{h(x)}_{h(x)}
 \end{equation} 
consecuentemente, por el Teorema  \ref{teo:neyman-fisher} (Neyman-Fisher), tenemos que $T$ es un estadístico suficiente para $\nu$.
\end{remark}

La familia exponencial va a ser ampliamente usada durante el curso, lo cual se debe a sus propiedades favorables para el análisis estadístico. Por ejemplo, el producto de dos distribuciones de la familia exponencial también pertenece a la familia exponencial. En efecto, consideremos dos VA $X_1,X_2,$ con distribuciones en la familia exponencial respectivamente dadas por
\begin{align}
	p_1(x_1) &= h_1(x_1)\expo{\theta_1T_1(x_1)-{A_1(\theta_1)}}\\
	p_2(x_2) &= h_2(x_2)\expo{\theta_2T_2(x_2)-{A_2(\theta_2)}}
\end{align}
si asumimos que estas VA son independientes, entonces densidad conjunta de $X=(X_1,X_2)\sim p$ está dada por
\begin{align}
	p(x) 	&= p_1(x_1) p_2(x_2) \nonumber \\ 
			&= \underbrace{h_1(x_1)h_2(x_2)}_{h(x)}\expo{\underbrace{[\theta_1,\theta_2]}_{\theta}\underbrace{
			\left[ \begin{array}{c}
			T_1(x_1)  \\
			T_2(x_2)  \end{array} \right]
			}_{T(x)} - \underbrace{\left(A_1(\theta_1)+A_2(\theta_2) \right)}_{A(\theta)}},
\end{align}
con lo que eligiendo $\theta=[\theta_1,\theta_2]$ y $T=[T_1,T_2]$, vemos que $X$ está dado por una distribución de la familia exponencial.  

Otra propiedad de las familia exponencial es la relación entre los momentos de la distribucion y el lognormalizador $A$. Denotando
\begin{equation}
	Q(\theta) =\expo{A(\theta)} = \int_\cX \expo{\theta T (x)}h(x)\dx,
\end{equation}
podemos observar que la derivada de $A(\theta)$ está dada por 
\begin{align}
	\frac{dA(\theta)}{d\theta} &= Q^{-1}(\theta)\frac{d Q(\theta)}{d\theta} \\ 
	&= \frac{\int_\cX T (x) \expo{\theta T (x)}h(x)\dx}{\int_\cX \expo{\theta T (x)}h(x)\dx} \nonumber\\
	&= \frac{\int_\cX T (x) \expo{\theta T (x) -A(\theta)}h(x)\dx}{\int_\cX \expo{\theta T (x)-A(\theta) }h(x)\dx} \quad\quad \cdot  A(\theta)/A(\theta) \nonumber\\
	&=\E{T(x)}  \nonumber
\end{align}

\begin{example}
Verificar para derivadas de orden superior (ejercicio)
\end{example}


\begin{remark}
Consideremos el mapa
\begin{equation}
    \theta \mapsto \mu = \frac{dA(\theta)}{d\theta} = \int_\cX T (x) \expo{\theta T (x) -A(\theta)}h(x)\dx =\E{T(x)}
\end{equation}
Para ciertas familias, este mapa es biyectivo (familia minimal $\rightarrow$ $A(\theta)$ estrictamente convexo), es decir, podemos expresar el modelo mediante los parámetros $\mu$ en vez de $\theta$, esto se llama \emph{mean parametrisation} (MP), manteniendo la relación 1-1 entre modelos a distintas parametrizaciones. La MP es fundamental en el problema de estimación: ¿por qué?

\end{remark}

\section{Ejercicios}

\begin{enumerate}
    \item Estudiemos el problema de estimación del área de un rectángulo. Consideremos un rectángulo de lados a,b desconocidos. Sea $X=(X_1,...,X_n)$ una MAS que representa el lado a) del rectángulo con $X_i\sim \mathcal{N}(a,\sigma^2)$ e $Y=(Y_1,...,Y_n)$ otra MAS que representa el lado b) del rectángulo con $Y_i\sim \mathcal{N}(b,\sigma^2)$, $\sigma$ conocido y X e Y independientes. Plantee el modelo paramétrico asociado y encuentre un estadístico suficiente. 
    \item Se desea estudiar el tiempo que demora en realizarse cierto proceso. Digamos que el proceso posee 2 etapas. Se consta de una MAS dada por $X=(X_1,...,X_n)$ donde cada $X_i\sim \Gamma(2,\theta)$ la distribución Gamma con $X_i$ el tiempo total que demora en realizarse un proceso completo (ambas etapas), $\theta$ es desconocido. Plantee el modelo parametrico y demuestre que $T(X)=\sum\limits_{i=1}^{n}X_i$ es un estadístico suficiente.
    
    \item Sea $X=(X_1,...,X_n)$ una MAS con $X_i\sim U(\alpha,\beta)$ donde $\alpha,\beta$ son desconocidos. Demuestre que $T(X)=\left(\min\limits_{i=1,...,n}{X_i},\max\limits_{i=1,...,n}{X_i}\right)$ es un estadístico suficiente para $(\alpha,\beta)$.
    
    \item Sea $X=(X_1,...,X_n)$ una MAS con $X_i\sim \Gamma(\alpha,\beta)$ la distribución Gamma dada por:
    \begin{equation}
        \nonumber 
        \Gamma(\alpha,\beta)\sim \frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1}e^{-\frac{1}{\beta}x}
    \end{equation}
    Demuestre que $T(X)=\left (\prod\limits_{i=1}^{n}x_i,\sum\limits_{i=1}^{n}x_i  \right )$ es un estadistico suficiente para $(\alpha,\beta)$
    
    \item Sea $X=(X_1,...,X_n)$ una MAS con función de densidad dada por:
    \begin{equation}
        \nonumber 
        f(x\mid \theta)=\frac{\theta}{(1+x)^{1+\theta}} , 0<x<\infty,\theta>0
    \end{equation}
    Con $\theta$ desconocido. Encuentre un estadístico suficiente.
    
    \item Sea $X=(X_1,...,X_n)$ una MAS con una función de densidad dada por:
    \begin{equation}
        \nonumber 
        f(x|\theta)=\theta x^{\theta -1},\theta>0,x\in (0,1)
    \end{equation}
    
    \begin{itemize}
        \item[(i)] ¿Es $\sum\limits_{i=1}^{n}X_i$ un estadístico suficiente para $\theta$?
        
        \item[(ii)] Encuentre un estadístico minimal suficiente para $\theta$
    
        \item[(iii)]  ¿Es $T'(X)=\sum\limits_{i=1}^{n}ln(X_i)$ un estadístico minimal para $\theta$?
        
        \item[(iv)] Encuentre un estadístico suficiente y completo para $\theta$.
        
        \item[(v)] Sea S(X) un estadístico ancilario para $\theta$. Encuentre la correlación entre S(X) y $T'(X)$.
        
    \end{itemize}
    
\end{enumerate}




